{

# general inputs

'env_name'      :   'halfcheetah-medium-v2',
'act_repeat'    :   1,
'seed'          :   123,
'num_iter'      :   2,
'eval_rollouts' :   5,
'num_models'    :   4,
'save_freq'     :   25,
'device'        :   'cuda',
'learn_reward'  :   False,
'reward_file'   :   './utils/reward_functions/gym_halfcheetah.py',
'pessimism_coef':   3.0,
'truncate_reward' : 0.0,
'exp_notes'     :   'Example to illustrate MOReL on a D4RL task.',

#behavior cloning
'bc_init'       : False,
'bc_epochs'     : 10,
'bc_batch_size' : 256,
'bc_lr'         : 1e-3,

# dynamics learning

'hidden_size'   :   (512, 512),
'activation'    :   'relu',
'fit_lr'        :   1e-3,
'fit_wd'        :   0.0,
'fit_mb_size'   :   256,
'fit_epochs'    :   25,
'refresh_fit'   :   False,
'max_steps'     :   1e8,

# NPG params

'policy_size'   :   (64, 64),
'step_size'     :   0.02,
'init_log_std'  :   -0.25,
'min_log_std'   :   -1.0,
'gamma'         :   0.999,
'gae_lambda'    :   0.97,
'update_paths'  :   50,
'start_state'   :   'init',
'horizon'       :   700,
'npg_hp'        :   dict(FIM_invert_args={'iters': 20, 'damping': 1e-3}),

# MPC Params
'mpc_params': {
'n_iters': 1,
'horizon': 10,
'num_particles': 100,
'init_cov': 0.02,
'gamma': 0.999,
'beta': 3.33,
'mixing_factor': 0.8, #0.6,
'td_lam': 0.97,
'step_size_mean': 1.0,
'sample_mode': 'best_mean',
'base_action': 'repeat',
'hotstart': True,
'shift_steps': 1,
'squash_fn': 'clamp',
'optimize_open_loop': False,
'filter_coeffs': [0.8, 0.2, 0.0],

}





}
